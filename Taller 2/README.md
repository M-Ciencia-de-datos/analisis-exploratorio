#  Taller 2: An谩lisis Exploratorio Univariable 

## Integrantes:

- Katherin Escobar
- Heberth Martinez
- Diana Mazuera
- Natalia Santamaria

## Introducci贸n

La base de datos de "Ciclo de sue帽o y productividad" analiza los h谩bitos de sue帽o y su impacto en la productividad, el estado de 谩nimo y los niveles de estr茅s.

Se cuenta con 5000 registros que abarcan personas entre los 18 y 60 a帽os de edad y sus distintos estilos de vida.

A continuaci贸n, est谩 la descripci贸n de cada columna de la base de datos:

![Data](assets/Data_descrip.PNG)

## Selecci贸n de variables

El an谩lisis realizado se hizo con el fin de identificar la caracter铆stica que tuviera una mayor relaci贸n con la variable objetivo "Productivity Score", por lo que se empez贸 una revisi贸n de la base de datos detectando que estamos frente a un problema de clasificaci贸n con variables no lineales, y no se cuentan con datos at铆picos, nulos o faltantes.

## Importancia de las variables

En la din谩mica actual, donde la tecnolog铆a y el ritmo acelerado dominan las rutinas, es crucial mantener la productividad y el bienestar.Teniendo en cuenta diferente literatura se observa que el sue帽o y el ejercio son fundamentales para la salud humana, afectando no solo el bienestar f铆sico sino tambi茅n la capacidad mental y cognitiva. Autores como Dement y Vaughan (1999), Van Dongen y otros (2003), Turner y otros (2007) demuestran que el sue帽o se relaciona con el desempe帽o cognitivo, la toma de decisiones, el razonamiento, la memoria, la soluci贸n de problemas, la atenci贸n e incluso los accidentes.

Tomando en cuenta no solo la literatura previa, sino tambi茅n la correlaci贸n de los datos, se determina que las mejores caracter铆sticas son "Total Sleep Hours" y "Exercise"

## Descripcion del c贸digo
<a target="_blank" href="https://colab.research.google.com/github/M-Ciencia-de-datos/analisis-exploratorio/blob/main/Taller%202/notebook.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

### Resumen del An谩lisis de Datos y Modelado con Regularizaci贸n

#### Carga y Exploraci贸n del Dataset

En la primera secci贸n, se carga el conjunto de datos en un `DataFrame` de `pandas`. Posteriormente, se realiza una exploraci贸n preliminar que incluye:

- Visualizaci贸n de las primeras filas con `df.head()`.
- Inspecci贸n de la estructura del dataset con `df.info()`.
- C谩lculo de estad铆sticas descriptivas con `df.describe()`.
- Detecci贸n de valores nulos mediante `df.isnull().sum()`.

Esto permite obtener una comprensi贸n inicial de las caracter铆sticas y posibles problemas del dataset.

#### An谩lisis Exploratorio de Datos

Se emplean diversas t茅cnicas de visualizaci贸n para entender la distribuci贸n de los datos y sus relaciones:

1. **Pairplot con Seaborn**: Se genera un `sns.pairplot(df)` para visualizar la distribuci贸n de variables y su relaci贸n entre s铆.
2. **Box Plot Interactivo con Plotly**: Se usa `plotly.express.box()` para representar la distribuci贸n de cada variable de manera interactiva.

#### C谩lculo de Correlaciones y Selecci贸n de Variables Relevantes

Se utiliza la correlaci贸n de Pearson para medir la relaci贸n entre las variables y la variable objetivo. El proceso es el siguiente:

- Se calcula la matriz de correlaci贸n con `df.corr()`.
- Se genera un `heatmap` con `seaborn` para visualizar los coeficientes de correlaci贸n.
- Se identifican las dos variables con mayor correlaci贸n con la variable objetivo.

#### Normalizaci贸n y Revisi贸n de Correlaciones

Para evaluar c贸mo afecta la normalizaci贸n a la correlaci贸n de las variables, se aplican dos m茅todos de escalado:

1. **StandardScaler**: Normaliza los datos a una distribuci贸n con media 0 y desviaci贸n est谩ndar 1.
2. **MinMaxScaler**: Escala los datos en un rango de 0 a 1.

Despu茅s de la normalizaci贸n, se recalcula la matriz de correlaci贸n para analizar posibles cambios.

#### An谩lisis Estad铆stico Descriptivo

Se calculan diversas m茅tricas estad铆sticas, incluyendo:

- Media y mediana.
- Moda y percentiles.
- Rango intercuart铆lico.

Estas estad铆sticas ayudan a comprender la dispersi贸n y distribuci贸n de los datos.

#### Modelado con Regresi贸n Log铆stica Regularizada

Se implementa una **regresi贸n log铆stica con ElasticNet y validaci贸n cruzada** (`LogisticRegressionCV`) para evaluar la importancia de las variables. El flujo es el siguiente:

1. **Divisi贸n en conjunto de entrenamiento y prueba** usando `train_test_split()` con `stratify=y`.
2. **Normalizaci贸n de datos** mediante `StandardScaler`.
3. **Entrenamiento del modelo** con regularizaci贸n ElasticNet (`penalty='elasticnet'`, `solver='saga'`).
4. **Evaluaci贸n de la precisi贸n del modelo** con `accuracy_score()`.
5. **Extracci贸n de los coeficientes** para identificar las variables m谩s relevantes.

## Interpretacion

\*Al evaluar las variables, se identifica que hay una baja correlaci贸n entre ellas. De esta manera, se procedi贸 a realizar una t茅cnica de regularizaci贸n para determinar cu谩les son las mejores caracter铆sticas de nuestra base de datos, confirmando que las mejores son "Total Sleep Hours", "Exercise", y "Work Hours". Datos que tienen concordancia con la matriz de correlaci贸n.

##  Conclusiones

1锔 **La correlaci贸n no siempre indica las mejores caracter铆sticas**

- Se debe identificar qu茅 tipo de relaci贸n hay entre las variables para escoger el m茅todo de correlaci贸n que m谩s se ajuste a los datos.
- Aunque las variables con mayor correlaci贸n con la variable objetivo pueden ser candidatas para el modelo, **no significa que sean las m谩s relevantes**.
- La regularizaci贸n en la regresi贸n log铆stica mostr贸 que otras variables tambi茅n pueden tener un impacto significativo, incluso si su correlaci贸n era menor.

2锔 **La normalizaci贸n no afecta el an谩lisis realizado**

- La transformaci贸n de los datos con **StandardScaler** y **MinMaxScaler** no cambi贸 la relaci贸n entre variables.
- La correlaci贸n entre las variables y la selecci贸n de caracter铆sticas se mantuvieron consistentes antes y despu茅s de la normalizaci贸n.

3锔 **El uso de t茅cnicas de regularizaci贸n es clave para la selecci贸n de variables**

- Aplicar **ElasticNet con validaci贸n cruzada** nos permiti贸 ver qu茅 variables realmente aportan informaci贸n 煤til al modelo.
- Algunas variables que parec铆an importantes por su correlaci贸n fueron descartadas por la regularizaci贸n.

4锔 **La validaci贸n cruzada ayuda a evitar el sobreajuste**

- Usar **cross-validation (CV)** nos asegur贸 que los resultados no dependieran solo de un conjunto de datos espec铆fico.
- Esto hizo que el modelo sea m谩s **robusto y generalizable**.

5锔 **El an谩lisis exploratorio de datos es fundamental antes de modelar**

- Visualizaciones como **pairplot (Seaborn)** y **boxplots interactivos (Plotly)** nos ayudaron a entender la distribuci贸n y relaciones de las variables.
- Detectamos posibles **outliers** y patrones que podr铆an afectar la predicci贸n.

6锔 **El preprocesamiento influye en el desempe帽o del modelo**

- T茅cnicas como la normalizaci贸n y la eliminaci贸n de valores nulos o inconsistentes **mejoran la estabilidad del modelo**.
- Un mal preprocesamiento puede afectar los coeficientes y la interpretaci贸n de la regresi贸n log铆stica.

7锔 **No todas las variables tienen impacto en la predicci贸n**

- Algunas variables fueron eliminadas por el modelo debido a su coeficiente **cercano a 0** tras la regularizaci贸n.
- Esto confirma la importancia de **no depender solo de la intuici贸n o la correlaci贸n**, sino de probar diferentes enfoques para seleccionar las mejores caracter铆sticas.

 **Conclusi贸n general:**  
El an谩lisis de correlaci贸n es un buen punto de partida, pero es fundamental utilizar t茅cnicas como la **regresi贸n con regularizaci贸n** para validar la importancia real de cada variable. La normalizaci贸n ayuda a mejorar la estabilidad num茅rica del modelo, pero no modifica el an谩lisis de importancia de caracter铆sticas. **Usar validaci贸n cruzada y preprocesamiento adecuado mejora la capacidad del modelo para generalizar su desempe帽o al contar nuevos datos.** 
