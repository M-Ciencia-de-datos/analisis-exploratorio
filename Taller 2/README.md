# üåô Taller 2: An√°lisis Exploratorio Univariable üí§

## Integrantes:

- Katherin Escobar
- Heberth Martinez
- Diana Mazuera
- Natalia Santamaria

## Introducci√≥n

La base de datos de "Ciclo de sue√±o y productividad" analiza los h√°bitos de sue√±o y su impacto en la productividad, el estado de √°nimo y los niveles de estr√©s.

Se cuenta con 5000 registros que abarcan personas entre los 18 y 60 a√±os de edad y sus distintos estilos de vida.

A continuaci√≥n, est√° la descripci√≥n de cada columna de la base de datos:

![Data](assets/Data_descrip.PNG)

## Selecci√≥n de variables

El an√°lisis realizado se hizo con el fin de identificar la caracter√≠stica que tuviera una mayor relaci√≥n con la variable objetivo "Productivity Score", por lo que se empez√≥ una revisi√≥n de la base de datos detectando que estamos frente a un problema de clasificaci√≥n con variables no lineales, y no se cuentan con datos at√≠picos, nulos o faltantes.

## Importancia de las variables

En la din√°mica actual, donde la tecnolog√≠a y el ritmo acelerado dominan las rutinas, es crucial mantener la productividad y el bienestar. Teniendo en cuenta diferente literatura se observa que el sue√±o y el ejercio son fundamentales para la salud humana, afectando no solo el bienestar f√≠sico sino tambi√©n la capacidad mental y cognitiva. Autores como Dement y Vaughan (1999), Van Dongen y otros (2003), Turner y otros (2007) demuestran que el sue√±o se relaciona con el desempe√±o cognitivo, la toma de decisiones, el razonamiento, la memoria, la soluci√≥n de problemas, la atenci√≥n e incluso los accidentes.

Tomando en cuenta no solo la literatura previa, sino tambi√©n la correlaci√≥n de los datos, se determina que las mejores caracter√≠sticas son "Total Sleep Hours" y "Exercise".

## Descripcion del c√≥digo

<a target="_blank" href="https://colab.research.google.com/github/M-Ciencia-de-datos/analisis-exploratorio/blob/main/Taller%202/notebook.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

### Resumen del An√°lisis de Datos y Modelado con Regularizaci√≥n

#### Carga y Exploraci√≥n del Dataset

En la primera secci√≥n, se carga el conjunto de datos en un `DataFrame` de `pandas`. Posteriormente, se realiza una exploraci√≥n preliminar que incluye:

- Visualizaci√≥n de las primeras filas con `df.head()`.
- Inspecci√≥n de la estructura del dataset con `df.info()`.
- C√°lculo de estad√≠sticas descriptivas con `df.describe()`.
- Detecci√≥n de valores nulos mediante `df.isnull().sum()`.

Esto permite obtener una comprensi√≥n inicial de las caracter√≠sticas y posibles problemas del dataset.

#### An√°lisis Exploratorio de Datos

Se emplean diversas t√©cnicas de visualizaci√≥n para entender la distribuci√≥n de los datos y sus relaciones:

1. **Pairplot con Seaborn**: Se genera un `sns.pairplot(df)` para visualizar la distribuci√≥n de variables y su relaci√≥n entre s√≠.
2. **Box Plot Interactivo con Plotly**: Se usa `plotly.express.box()` para representar la distribuci√≥n de cada variable de manera interactiva.

#### C√°lculo de Correlaciones y Selecci√≥n de Variables Relevantes

Se utiliza la correlaci√≥n de Pearson para medir la relaci√≥n entre las variables y la variable objetivo. El proceso es el siguiente:

- Se calcula la matriz de correlaci√≥n con `df.corr()`.
- Se genera un `heatmap` con `seaborn` para visualizar los coeficientes de correlaci√≥n.
- Se identifican las dos variables con mayor correlaci√≥n con la variable objetivo.

#### Normalizaci√≥n y Revisi√≥n de Correlaciones

Para evaluar c√≥mo afecta la normalizaci√≥n a la correlaci√≥n de las variables, se aplican dos m√©todos de escalado:

1. **StandardScaler**: Normaliza los datos a una distribuci√≥n con media 0 y desviaci√≥n est√°ndar 1.
2. **MinMaxScaler**: Escala los datos en un rango de 0 a 1.

Despu√©s de la normalizaci√≥n, se recalcula la matriz de correlaci√≥n para analizar posibles cambios.

#### An√°lisis Estad√≠stico Descriptivo

Se calculan diversas m√©tricas estad√≠sticas, incluyendo:

- Media y mediana.
- Moda y percentiles.
- Rango intercuart√≠lico.

Estas estad√≠sticas ayudan a comprender la dispersi√≥n y distribuci√≥n de los datos.

#### Modelado con Regresi√≥n Log√≠stica Regularizada

Se implementa una **regresi√≥n log√≠stica con ElasticNet y validaci√≥n cruzada** (`LogisticRegressionCV`) para evaluar la importancia de las variables. El flujo es el siguiente:

1. **Divisi√≥n en conjunto de entrenamiento y prueba** usando `train_test_split()` con `stratify=y`.
2. **Normalizaci√≥n de datos** mediante `StandardScaler`.
3. **Entrenamiento del modelo** con regularizaci√≥n ElasticNet (`penalty='elasticnet'`, `solver='saga'`).
4. **Evaluaci√≥n de la precisi√≥n del modelo** con `accuracy_score()`.
5. **Extracci√≥n de los coeficientes** para identificar las variables m√°s relevantes.

### Resumen del An√°lisis de Datos y Modelado con Regularizaci√≥n

#### Carga y Exploraci√≥n del Dataset

En la primera secci√≥n, se carga el conjunto de datos en un `DataFrame` de `pandas`. Posteriormente, se realiza una exploraci√≥n preliminar que incluye:

- Visualizaci√≥n de las primeras filas con `df.head()`.
- Inspecci√≥n de la estructura del dataset con `df.info()`.
- C√°lculo de estad√≠sticas descriptivas con `df.describe()`.
- Detecci√≥n de valores nulos mediante `df.isnull().sum()`.

Esto permite obtener una comprensi√≥n inicial de las caracter√≠sticas y posibles problemas del dataset.

#### An√°lisis Exploratorio de Datos

Se emplean diversas t√©cnicas de visualizaci√≥n para entender la distribuci√≥n de los datos y sus relaciones:

1. **Pairplot con Seaborn**: Se genera un `sns.pairplot(df)` para visualizar la distribuci√≥n de variables y su relaci√≥n entre s√≠.
2. **Box Plot Interactivo con Plotly**: Se usa `plotly.express.box()` para representar la distribuci√≥n de cada variable de manera interactiva.

#### C√°lculo de Correlaciones y Selecci√≥n de Variables Relevantes

Se utiliza la correlaci√≥n de Pearson para medir la relaci√≥n entre las variables y la variable objetivo. El proceso es el siguiente:

- Se calcula la matriz de correlaci√≥n con `df.corr()`.
- Se genera un `heatmap` con `seaborn` para visualizar los coeficientes de correlaci√≥n.
- Se identifican las dos variables con mayor correlaci√≥n con la variable objetivo.

#### Normalizaci√≥n y Revisi√≥n de Correlaciones

Para evaluar c√≥mo afecta la normalizaci√≥n a la correlaci√≥n de las variables, se aplican dos m√©todos de escalado:

1. **StandardScaler**: Normaliza los datos a una distribuci√≥n con media 0 y desviaci√≥n est√°ndar 1.
2. **MinMaxScaler**: Escala los datos en un rango de 0 a 1.

Despu√©s de la normalizaci√≥n, se recalcula la matriz de correlaci√≥n para analizar posibles cambios.

#### An√°lisis Estad√≠stico Descriptivo

Se calculan diversas m√©tricas estad√≠sticas, incluyendo:

- Media y mediana.
- Moda y percentiles.
- Rango intercuart√≠lico.

Estas estad√≠sticas ayudan a comprender la dispersi√≥n y distribuci√≥n de los datos.

#### Modelado con Regresi√≥n Log√≠stica Regularizada

Se implementa una **regresi√≥n log√≠stica con ElasticNet y validaci√≥n cruzada** (`LogisticRegressionCV`) para evaluar la importancia de las variables. El flujo es el siguiente:

1. **Divisi√≥n en conjunto de entrenamiento y prueba** usando `train_test_split()` con `stratify=y`.
2. **Normalizaci√≥n de datos** mediante `StandardScaler`.
3. **Entrenamiento del modelo** con regularizaci√≥n ElasticNet (`penalty='elasticnet'`, `solver='saga'`).
4. **Evaluaci√≥n de la precisi√≥n del modelo** con `accuracy_score()`.
5. **Extracci√≥n de los coeficientes** para identificar las variables m√°s relevantes.

## Interpretacion

El an√°lisis se divide en tres etapas. La primera etapa, se enfoca en comprender las relaciones lineales entre diversas variables y la puntuaci√≥n de productividad. Se observ√≥ que variables como las horas de sue√±o total, el ejercicio y la ingesta de cafe√≠na presentaban correlaciones d√©biles con la productividad, sugiriendo, que, por s√≠ solas, no logran predecir, de manera lineal la productividad.

Para profundizar en el estudio, se analizaron gr√°ficos de dispersi√≥n que representaban la relaci√≥n entre "Exercise (mins/day)" y "Productivity Score", as√≠ como entre "Total Sleep Hours" y "Productivity Score". En ambos casos, se observ√≥ una falta de correlaci√≥n lineal, reforzando la idea de que ni la cantidad de ejercicio ni la cantidad de sue√±o, por s√≠ solas, son determinantes clave de la productividad en el trabajo. Estos resultados sugieren que otros factores podr√≠an estar influyendo en la productividad.

En la segunda etapa, se aplicaron t√©cnicas de normalizaci√≥n de datos para evaluar cambios en los valores de las variables y su impacto en la correlaci√≥n. Al comparar la matriz de correlaci√≥n normalizada con la matriz de correlaci√≥n original, se nota que las correlaciones entre la productividad y el ejercicio, as√≠ como entre la productividad y las horas de sue√±o, se mantienen despu√©s de la normalizaci√≥n. Lo anterior, refuerza la idea de que estas variables no tienen una relaci√≥n lineal fuerte con la productividad. Adem√°s, los valores de correlaci√≥n no experimentaron cambios significativos que alteraran las conclusiones principales, lo que indica que las relaciones entre las variables son ‚Äúrobustas‚Äù y no se ven afectadas significativamente por la escala de los datos.

Por √∫ltimo, en la tercera etapa, se utiliz√≥ un modelo de Regresi√≥n Log√≠stica con ElasticNet y validaci√≥n cruzada para identificar las variables m√°s relevantes en la predicci√≥n de la variable objetivo. El modelo ElasticNet seleccion√≥ "Screen Time Before Bed (mins)" y "Total Sleep Hours" como las variables m√°s importantes para predecir la productividad. Sin embargo, la precisi√≥n del modelo en el conjunto de prueba fue del 11%, lo que sugiere que, aunque estas variables tienen cierto impacto, el modelo lineal no captura completamente la relaci√≥n. La baja precisi√≥n del modelo indica que se necesitan enfoques adicionales para mejorar la predicci√≥n de la variable objetivo.

## üìå Conclusiones

1Ô∏è‚É£ **La correlaci√≥n no siempre indica las mejores caracter√≠sticas**

- Se debe identificar qu√© tipo de relaci√≥n hay entre las variables para escoger el m√©todo de correlaci√≥n que m√°s se ajuste a los datos.
- Aunque las variables con mayor correlaci√≥n con la variable objetivo pueden ser candidatas para el modelo, **no significa que sean las m√°s relevantes**.
- La regularizaci√≥n en la regresi√≥n log√≠stica mostr√≥ que otras variables tambi√©n pueden tener un impacto significativo, incluso si su correlaci√≥n era menor.

2Ô∏è‚É£ **La normalizaci√≥n no afecta el an√°lisis realizado**

- La transformaci√≥n de los datos con **StandardScaler** y **MinMaxScaler** no cambi√≥ la relaci√≥n entre variables.
- La correlaci√≥n entre las variables y la selecci√≥n de caracter√≠sticas se mantuvieron consistentes antes y despu√©s de la normalizaci√≥n.

3Ô∏è‚É£ **El uso de t√©cnicas de regularizaci√≥n es clave para la selecci√≥n de variables**

- Aplicar **ElasticNet con validaci√≥n cruzada** nos permiti√≥ ver qu√© variables realmente aportan informaci√≥n √∫til al modelo.
- Algunas variables que parec√≠an importantes por su correlaci√≥n fueron descartadas por la regularizaci√≥n.

4Ô∏è‚É£ **La validaci√≥n cruzada ayuda a evitar el sobreajuste**

- Usar **cross-validation (CV)** nos asegur√≥ que los resultados no dependieran solo de un conjunto de datos espec√≠fico.
- Esto hizo que el modelo sea m√°s **robusto y generalizable**.

5Ô∏è‚É£ **El an√°lisis exploratorio de datos es fundamental antes de modelar**

- Visualizaciones como **pairplot (Seaborn)** y **boxplots interactivos (Plotly)** nos ayudaron a entender la distribuci√≥n y relaciones de las variables.
- Detectamos posibles **outliers** y patrones que podr√≠an afectar la predicci√≥n.

6Ô∏è‚É£ **El preprocesamiento influye en el desempe√±o del modelo**

- T√©cnicas como la normalizaci√≥n y la eliminaci√≥n de valores nulos o inconsistentes **mejoran la estabilidad del modelo**.
- Un mal preprocesamiento puede afectar los coeficientes y la interpretaci√≥n de la regresi√≥n log√≠stica.

7Ô∏è‚É£ **No todas las variables tienen impacto en la predicci√≥n**

- Algunas variables fueron eliminadas por el modelo debido a su coeficiente **cercano a 0** tras la regularizaci√≥n.
- Esto confirma la importancia de **no depender solo de la intuici√≥n o la correlaci√≥n**, sino de probar diferentes enfoques para seleccionar las mejores caracter√≠sticas.

üîé **Conclusi√≥n general:**  
El an√°lisis de correlaci√≥n es un buen punto de partida, pero es fundamental utilizar t√©cnicas como la **regresi√≥n con regularizaci√≥n** para validar la importancia real de cada variable. La normalizaci√≥n ayuda a mejorar la estabilidad num√©rica del modelo, pero no modifica el an√°lisis de importancia de caracter√≠sticas. **Usar validaci√≥n cruzada y preprocesamiento adecuado mejora la capacidad del modelo para generalizar su desempe√±o al contar nuevos datos.** üöÄ
